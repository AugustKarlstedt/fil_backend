{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ad97cc",
   "metadata": {},
   "source": [
    "# The FIL Backend for Triton: Deployment Details and FAQs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This example notebook focuses on the technical details of deploying tree-based models with the FIL Backend for Triton. It is organized as a series of FAQs followed by example code providing a practical illustration of the corresponding FAQ section.\n",
    "\n",
    "The goal of this notebook is to offer information that goes beyond the basics and provide answers to practical questions that may arise when attempting a real-world deployment with the FIL backend. If you are a complete newcomer to the FIL backend and are looking for a short introduction to the basics of what the FIL backend is and how to use it, you are encouraged to check out [this introductory notebook](https://github.com/triton-inference-server/fil_backend/blob/main/notebooks/categorical-fraud-detection/Fraud_Detection_Example.ipynb).\n",
    "\n",
    "While we do provide training code for example models, training models is *not* the subject of this notebook, and we will provide little detail on training. Instead, you are encouraged to use your own model(s) and data with this notebook to get a realistic picture of how your model will perform with Triton.\n",
    "\n",
    "## Hardware Pre-Requisites\n",
    "Most of this notebook is designed to run either on CPU or GPU. Sections that will only run on GPU will be marked in $\\color{#76b900}{\\text{green}}$. To guarantee that all cells will execute correctly if a GPU is not available, change `USE_GPU` in the following cell to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58491c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b761ac",
   "metadata": {},
   "source": [
    "\n",
    "## Software Pre-Requisites\n",
    "\n",
    "TODO\n",
    "\n",
    "### Bring Your Own Model\n",
    "If you are bringing your own model(s) to use with this notebook, you will need only [Docker](https://docs.docker.com/engine/install/), Numpy, and Triton's Python client package. The following command can be used to install the Python dependencies in any Python environment:\n",
    "```bash\n",
    "pip install numpy tritonclient[all]\n",
    "```\n",
    "Note that the Triton client package also requires that `libb64` be available. This can be installed on Debian-based systems via\n",
    "```bash\n",
    "sudo apt install libb64-0d\n",
    "```\n",
    "\n",
    "### Train an Example Model\n",
    "Training the example models in this notebook requires more extensive dependencies. You can install them all with the following conda environment file\n",
    "```yaml\n",
    "---\n",
    "name: triton_example\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - nvidia\n",
    "  - rapidsai\n",
    "dependencies:\n",
    "  - cudatoolkit=11.4\n",
    "  - cudf=21.12\n",
    "  - cuml=21.12\n",
    "  - cupy\n",
    "  - jupyter\n",
    "  - kaggle\n",
    "  - matplotlib\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - pip\n",
    "  - python=3.8\n",
    "  - scikit-learn\n",
    "  - pip:\n",
    "      - tritonclient[all]\n",
    "      - xgboost>=1.5,<1.6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86510373",
   "metadata": {},
   "source": [
    "# FAQ 1: What can I deploy with the FIL backend?\n",
    "The first thing you will need to begin using the FIL backend is a serialized model file. The FIL backend supports **tree-based** models serialized to formats from a variety of frameworks, including the following:\n",
    "\n",
    "## XGBoost JSON and binary models\n",
    "XGBoost uses two serialization formats, both of which are natively supported by the FIL backend. All XGBoost models except for multi-output regression models are supported.\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>VERSION NOTE:</b> Categorical variable support was added to XGBoost 1.5 as an experimental feature. The FIL backend has supported categorical variables since version 21.11.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>VERSION NOTE:</b> The XGBoost JSON format changed in XGBoost 1.6. The first version of the FIL backend to support these JSON changes will be 22.07.\n",
    "</div>\n",
    "\n",
    "## LightGBM text models\n",
    "LightGBM's text serialization format is natively supported for all LightGBM model types except for multi-output regression models.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>VERSION NOTE:</b> Models trained on categorical variables have been supported since version 21.11 of the backend\n",
    "</div>\n",
    "\n",
    "## Scikit-Learn/cuML tree models and other Treelite-supported models\n",
    "\n",
    "The FIL backend supports the following model types from Scikit-Learn/cuML:\n",
    "- GradientBoostingClassifier\n",
    "- GradientBoostingRegressor\n",
    "- IsolationForest\n",
    "- RandomForestClassifier\n",
    "- RandomForestRegressor\n",
    "- ExtraTreesClassifier\n",
    "- ExtraTreesRegressor\n",
    "\n",
    "Since Scikit-Learn and cuML do not have native serialization formats for these models (instead relying on e.g. Pickle), we use Treelite's checkpoint format to support these models. This also means that *any* framework that can export to Treelite's checkpoint format will be supported by the FIL backend. As part of this notebook, we will provide an example of how to save a Scikit-Learn or cuML model to a Treelite checkpoint.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>VERSION NOTE:</b> Treelite's checkpoint format provides no forward/backward compatibility guarantees. It is therefore <b>strongly recommended</b> that you save Scikit-Learn and cuML models to Pickle so that they can be reconverted as needed. The table below shows the version of Treelite which <b>must</b> be used with each version of the FIL backend.\n",
    "    \n",
    "<table>\n",
    "    <thead>\n",
    "        <tr><th>FIL Backend Version</th><th>Treelite</th></tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><td>21.08</td><td>1.3.0</td></tr>\n",
    "        <tr><td>21.09-21.10</td><td>2.0.0</td></tr>\n",
    "        <tr><td>21.11-22.02</td><td>2.1.0</td></tr>\n",
    "        <tr><td>22.03-22.06</td><td>2.3.0</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### FAQ 1.1 Can I deploy non-tree Scikit-Learn models like LinearRegression?\n",
    "No. The FIL backend only supports tree models and will continue to support only tree models in the future. Support for other model types may eventually be added to Triton via another backend.\n",
    "\n",
    "### FAQ 1.2 Can I deploy Scikit-Learn/cuML Pipelines with the FIL backend?\n",
    "No. If you wish to create pipelines of different models in Triton, check out Triton's [Python backend](https://github.com/triton-inference-server/python_backend#python-backend), which allows users to connect models supported by other backends with arbitrary Python logic.\n",
    "\n",
    "### FAQ 1.3 Can I deploy Scikit-Learn/cuML models serialized with Pickle?\n",
    "Pickle-serialized models can be converted to Treelite's checkpoint format using a script provided with the FIL Backend. This script is [documented here](https://github.com/triton-inference-server/fil_backend/blob/main/SKLearn_and_cuML.md#converting-to-treelite-checkpoints), and an example of its use will be included with this notebook. **Pickle models MUST be converted to Treelite checkpoints. They CANNOT be used directly by the FIL backend.**\n",
    "\n",
    "### FAQ 1.4 Can I deploy Scikit-Learn/cuML models serialized with Joblib?\n",
    "JobLib-serialized models can be loaded in Python and serialized to Treelite checkpoints. At the moment, the conversion scripts for Pickle-serialized models do **not** work with Joblib, but support for Joblib will be added with a later version. **Joblib models MUST be converted to Treelite checkpoints. They CANNOT be used directly by the FIL backend.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038cd3fa",
   "metadata": {},
   "source": [
    "# Example 1: Model Serialization\n",
    "\n",
    "In the following example code snippets, we will demonstrate how model serialization works for each of the supported model types. In the cell below, indicate the type of model you would like to use.\n",
    "\n",
    "If you are bringing your own model, please also provide the path to the serialized model. Otherwise, a model will be trained on random data in your selected format.\n",
    "\n",
    "In addition to information on where and how the model is stored, we'll use the following cell to gather a bit of metadata on the model which we'll need later on including the number of features the model expects and the number of classes it outputs. If you are using a regression model, use `1` for the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "977f2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowed values for MODEL_FORMAT are xgboost_json, xgboost_bin, lightgbm, skl_pkl, cuml_pkl, skl_joblib,\n",
    "# and treelite\n",
    "MODEL_FORMAT = 'xgboost_json'\n",
    "\n",
    "# If a path is provided to a model in the specified format, that model will be used for the following examples.\n",
    "# Otherwise, if MODEL_PATH is left as None, a model will be trained and stored to a default location.\n",
    "MODEL_PATH = None\n",
    "\n",
    "# Set this value to the number of features (columns) in your dataset\n",
    "NUM_FEATURES = 32\n",
    "\n",
    "# Set this value to the number of possible output classes or 1 for regression models\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c5b43",
   "metadata": {},
   "source": [
    "## Model Training/Loading\n",
    "\n",
    "In this section, if a model path has been provided, we will load the model so that we can compare its output to what we get from Triton later in the notebook. If a model path has **not** been provided, a model of the indicated type will be trained and serialized to a default location. We will not provide detail or commentary on training, since this is not the focus of this notebook. Consult documentation or examples for your chosen framework if you would like to learn more about the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49760159",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e483d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "# Create random dataset. Even if we do not use this dataset for training, we will use it for testing later.\n",
    "# If you would like to use a real dataset, load it here into X and y Pandas dataframes\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=NUM_FEATURES,\n",
    "    n_informative=max(NUM_FEATURES // 3, 1),\n",
    "    n_classes=NUM_CLASSES,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03943d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameters for any models we need to train\n",
    "NUM_TREES = 500\n",
    "MAX_DEPTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8a2a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "# XGBoost\n",
    "def train_xgboost(X, y, n_trees=NUM_TREES, max_depth=MAX_DEPTH):\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    if USE_GPU:\n",
    "        tree_method = 'gpu_hist'\n",
    "        predictor = 'gpu_predictor'\n",
    "    else:\n",
    "        tree_method = 'hist'\n",
    "        predictor = 'cpu_predictor'\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        eval_metric='error',\n",
    "        objective='binary:logistic',\n",
    "        tree_method=tree_method,\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_trees,\n",
    "        use_label_encoder=False,\n",
    "        predictor=predictor\n",
    "    )\n",
    "    \n",
    "    return model.fit(X, y)\n",
    "\n",
    "def train_lightgbm(X, y, n_trees=NUM_TREES, max_depth=MAX_DEPTH):\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    lgb_data = lgb.Dataset(X, y)\n",
    "    \n",
    "    if classes <= 2:\n",
    "        classes = 1\n",
    "        objective = 'binary'\n",
    "        metric = 'binary_logloss'\n",
    "    else:\n",
    "        objective = 'multiclass'\n",
    "        metric = 'multi_logloss'\n",
    "    training_params = {\n",
    "        'metric': metric,\n",
    "        'objective': objective,\n",
    "        'num_class': NUM_CLASSES,\n",
    "        'max_depth': max_depth,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    return lgb.train(training_params, lgb_data, n_trees)\n",
    "\n",
    "def train_cuml(X, y, n_trees=NUM_TREES, max_depth=MAX_DEPTH):\n",
    "    from cuml.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(\n",
    "        max_depth=max_depth, n_estimators=n_trees, random_state=RANDOM_SEED\n",
    "    )\n",
    "    return model.fit(X, y)\n",
    "\n",
    "def train_skl(X, y, n_trees=NUM_TREES, max_depth=MAX_DEPTH):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(\n",
    "        max_depth=max_depth, n_estimators=n_trees, random_state=RANDOM_SEED\n",
    "    )\n",
    "    return model.fit(X, y)\n",
    "    \n",
    "    \n",
    "if MODEL_FORMAT in ('xgboost_json', 'xgboost_bin'):\n",
    "    if MODEL_PATH is not None:\n",
    "        # Load model just as a reference for later\n",
    "        import xgboost as xgb\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(MODEL_PATH)\n",
    "        print('Congratulations! Your model is already in a natively-supported format')\n",
    "    else:\n",
    "        model = train_xgboost(X, y)\n",
    "elif MODEL_FORMAT == 'lightgbm':\n",
    "    if MODEL_PATH is not None:\n",
    "        # Load model just as a reference for later\n",
    "        import lightgbm as lgb\n",
    "        model = lgb.Booster(model_file=MODEL_PATH)\n",
    "        print('Congratulations! Your model is already in a natively-supported format')\n",
    "    else:\n",
    "        model = train_lightgbm(X, y)\n",
    "elif MODEL_FORMAT in ('cuml_pkl', 'skl_pkl', 'cuml_joblib', 'skl_joblib'):\n",
    "    if MODEL_PATH is not None:\n",
    "        if MODEL_FORMAT in ('cuml_pkl', 'skl_pkl'):\n",
    "            # Load model just as a reference for later\n",
    "            import pickle\n",
    "            model = pickle.load(MODEL_PATH)\n",
    "            print(\n",
    "                \"While pickle files are not natively supported, we will use a script to\"\n",
    "                \" convert your model to a Treelite checkpoint later in this notebook.\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"Loading model from joblib file in order to convert it to Treelite checkpoint...\")\n",
    "            import joblib\n",
    "            model = joblib.load(MODEL_PATH)\n",
    "    elif MODEL_FORMAT.startswith('cuml'):\n",
    "        model = train_cuml(X, y)\n",
    "    elif MODEL_FORMAT.startswith('skl'):\n",
    "        model = train_skl(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b5b9e4",
   "metadata": {},
   "source": [
    "## The Model Repository\n",
    "Triton expects models to be stored in a specific directory structure. We will go ahead and create this directory structure now and serialize our models directly into the final directory or copy the serialized model there if the trained model was provided.\n",
    "\n",
    "Each model requires a configuration file stored in `$MODEL_REPO/$MODEL_NAME/config.pbtxt`, and a model file stored in `$MODEL_REPO/$MODEL_NAME/$MODEL_VERSION/$MODEL_FILENAME`. Note that Triton supports storing multiple versions of a model directories with different `$MODEL_VERSION` numbers starting from `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39eb4a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "MODEL_NAME = 'example_model'\n",
    "MODEL_VERSION = 1\n",
    "MODEL_REPO = os.path.abspath('data/model_repository')\n",
    "MODEL_DIR = os.path.join(MODEL_REPO, MODEL_NAME)\n",
    "VERSIONED_DIR = os.path.join(MODEL_DIR, str(MODEL_VERSION))\n",
    "\n",
    "os.makedirs(VERSIONED_DIR, exist_ok=True)\n",
    "\n",
    "# We will use the following variables to record information from the serialization\n",
    "# process that we will require later\n",
    "model_path = None\n",
    "model_format = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b396b3",
   "metadata": {},
   "source": [
    "## Example 1.1: Serializing an XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54bb67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FORMAT == 'xgboost_json':\n",
    "    # This is the default filename expected for XGBoost JSON models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_basename = 'xgboost.json'\n",
    "    model_path = os.path.join(VERSIONED_DIR, model_basename)\n",
    "    \n",
    "    model_format = 'xgboost_json'\n",
    "elif MODEL_FORMAT == 'xgboost_bin':\n",
    "    # This is the default filename expected for XGBoost binary models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_basename = 'xgboost.model'\n",
    "    model_path = os.path.join(VERSIONED_DIR, model_basename)\n",
    "    \n",
    "    # This is the format name Triton uses to indicate XGBoost binary models\n",
    "    model_format = 'xgboost'\n",
    "\n",
    "if MODEL_FORMAT.startswith('xgboost'):\n",
    "    if MODEL_PATH is not None:  # Just need to copy existing file...\n",
    "        shutil.copy(MODEL_PATH, model_path)\n",
    "    else:\n",
    "        model.save_model(model_path)  # XGB derives format from extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d87bb",
   "metadata": {},
   "source": [
    "## Example 1.2 Serializing a LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32c82643",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FORMAT == 'lightgbm':\n",
    "    # This is the default filename expected for LightGBM text models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_basename = 'model.txt'\n",
    "    model_path = os.path.join(VERSIONED_DIR, model_basename)\n",
    "    \n",
    "    model_format = MODEL_FORMAT\n",
    "    \n",
    "    if MODEL_PATH is not None:  # Just need to copy existing file...\n",
    "        shutil.copy(MODEL_PATH, model_path)\n",
    "    else:\n",
    "        model.save_model(model_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7eb0d",
   "metadata": {},
   "source": [
    "## Example 1.3 Serializing an in-memory Scikit-Learn model\n",
    "<a id='example_1.3'></a>The following will show how to serialize a SKL model from Python directly to a Treelite checkpoint format. This could be a model that you have just trained or a model that you have e.g. loaded from Joblib. Again it is strongly recommended that you **save trained models in Pickle/Joblib as well as Treelite** since Treelite provides no compatibility guarantees between versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110924ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and MODEL_FORMAT.startswith('skl'):\n",
    "    import pickle\n",
    "    archival_path = os.path.join(VERSIONED_DIR, 'model.pkl')\n",
    "    pickle.dump(model, archival_path)  # Create archival pickled version\n",
    "    \n",
    "    # This is the default filename expected for Treelite checkpoint models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_basename = 'checkpoint.tl'\n",
    "    model_path = os.path.join(VERSIONED_DIR, model_basename)\n",
    "    \n",
    "    model_format = 'treelite_checkpoint'\n",
    "    \n",
    "    import treelite\n",
    "    tl_model = treelite.sklearn.import_model(model)\n",
    "    tl_model.serialize(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70cc57",
   "metadata": {},
   "source": [
    "## Example 1.4 Serializing an in-memory cuML model\n",
    "<a id='example_1.4'></a>The following will show how to serialize a cuML model from Python directly to a Treelite checkpoint format. This could be a model that you have just trained or a model that you have e.g. loaded from Joblib. Again it is strongly recommended that you **save trained models in Pickle/Joblib as well as Treelite** since Treelite provides no compatibility guarantees between versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16c0fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and MODEL_FORMAT.startswith('cuml'):\n",
    "    import pickle\n",
    "    archival_path = os.path.join(VERSIONED_DIR, 'model.pkl')\n",
    "    pickle.dump(model, archival_path)  # Create archival pickled version\n",
    "    \n",
    "    # This is the default filename expected for Treelite checkpoint models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_basename = 'checkpoint.tl'\n",
    "    model_path = os.path.join(VERSIONED_DIR, model_basename)\n",
    "    \n",
    "    model_format = 'treelite_checkpoint'\n",
    "    \n",
    "    model.convert_to_treelite_model().to_treelite_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a73ee",
   "metadata": {},
   "source": [
    "## Example 1.5 Converting a pickled Scikit-Learn model\n",
    "For convenience, the FIL backend provides a script which can be used to convert a pickle file containing a Scikit-Learn model directly to a Treelite checkpoint file. If you do not have access to that script or prefer to work directly from Python, you can always load the pickled model into memory and then serialize it as in [Example 1.3](#example_1.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bbf2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_PATH is not None and MODEL_FORMAT == 'skl_pkl':\n",
    "    archival_path = os.path.join(VERSIONED_DIR, 'model.pkl')\n",
    "    shutil.copy(MODEL_PATH, archival_path)\n",
    "    \n",
    "    !../../scripts/convert_sklearn {archival_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adadf2ce",
   "metadata": {},
   "source": [
    "## Example 1.6 Converting a pickled cuML model\n",
    "For convenience, the FIL backend provides a script which can be used to convert a pickle file containing a cuML model directly to a Treelite checkpoint file. If you do not have access to that script or prefer to work directly from Python, you can always load the pickled model into memory and then serialize it as in [Example 1.4](#example_1.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b22bfb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_PATH is not None and MODEL_FORMAT == 'cuml_pkl':\n",
    "    archival_path = os.path.join(VERSIONED_DIR, 'model.pkl')\n",
    "    shutil.copy(MODEL_PATH, archival_path)\n",
    "    \n",
    "    !python ../../scripts/convert_cuml.py {archival_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f5015",
   "metadata": {},
   "source": [
    "# FAQ 2: How do I execute models on CPU only? On GPU?\n",
    "\n",
    "In addition to a serialized model file, you must provide a `config.pbtxt` configuration file for each model you wish to serve with the FIL backend for Triton. Within that file, it is possible to specify whether a model will run on CPU or GPU and how many instances of the model you wish to serve. For example, adding the following entry to the configuration file will create one instance of the model for each available GPU and run those instances each on their own dedicated GPU:\n",
    "\n",
    "```pbtxt\n",
    "  instance_group [{ kind: KIND_GPU }]\n",
    "```\n",
    "\n",
    "If you wish to instead run exactly three instances on CPU, the following entry can be used:\n",
    "```pbtxt\n",
    "  instance_group [\n",
    "    {\n",
    "      count: 3\n",
    "      kind: KIND_CPU\n",
    "    }\n",
    "  ]\n",
    "```\n",
    "\n",
    "In the following example, we will create a configuration file that can be used to serve your model on either CPU or GPU depending on the value of the `USE_GPU` flag set earlier in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c067fd",
   "metadata": {},
   "source": [
    "# Example 2: Generating a configuration file\n",
    "\n",
    "Based on the information provided about your model in previous cells, we can now construct a `config.pbtxt` that can be used to run that model on Triton. We will generate the configuration text and save it to the appropriate location.\n",
    "\n",
    "For full information on configuration options, check out the FIL backend [documentation](https://github.com/triton-inference-server/fil_backend#configuration). For a detailed example of configuration file construction, you can also check out the [introductory notebook](https://nbviewer.org/github/triton-inference-server/fil_backend/blob/main/notebooks/categorical-fraud-detection/Fraud_Detection_Example.ipynb#The-Configuration-File)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c1be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum size in bytes for input and output arrays\n",
    "MAX_MEMORY_BYTES = 60_000_000\n",
    "bytes_per_sample = (NUM_FEATURES + NUM_CLASSES) * 4\n",
    "max_batch_size = MAX_MEMORY_BYTES // bytes_per_sample\n",
    "\n",
    "# Select deployment hardware (GPU or CPU)\n",
    "if USE_GPU:\n",
    "    instance_kind = 'KIND_GPU'\n",
    "else:\n",
    "    instance_kind = 'KIND_CPU'\n",
    "\n",
    "config_text = f\"\"\"backend: \"fil\",\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {NUM_FEATURES} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"xgboost_json\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"false\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"AUTO\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{}}\"\"\"\n",
    "\n",
    "config_path = os.path.join(MODEL_DIR, 'config.pbtxt')\n",
    "with open(config_path, 'w') as file_:\n",
    "    file_.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e051c189",
   "metadata": {},
   "source": [
    "# FAQ 3: How can I quickly test configuration options?\n",
    "Sometimes it is useful to be able to quickly iterate on the options available in the `config.pbtxt` file for your model. While it is not recommended for production deployments, Triton offers a \"polling\" mode which will automatically reload models when their configurations change. To use this option, launch the server with the `--model-control-mode=poll` flag. After changing the configuration, wait a few seconds for the model to reload, and then Triton will be ready to handle requests with the new configuration.\n",
    "\n",
    "# Example 3: Launching the Triton server with polling mode\n",
    "In the following cell, we will launch the server with the model repository we set up previously in this notebook. We will use pulling mode in order to allow us to tweak the configuration file and observe the impact of our changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "193612d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find image 'nvcr.io/nvidia/tritonserver:22.05-py3' locally\n",
      "22.05-py3: Pulling from nvidia/tritonserver\n",
      "\n",
      "\u001b[1B17ec1767: Already exists \n",
      "\u001b[1B80b25883: Pulling fs layer \n",
      "\u001b[1B31827455: Pulling fs layer \n",
      "\u001b[1B550cd86a: Pulling fs layer \n",
      "\u001b[1B52cc2849: Pulling fs layer \n",
      "\u001b[1Bc00269e8: Pulling fs layer \n",
      "\u001b[1Bb6f81809: Pulling fs layer \n",
      "\u001b[1Befd7921a: Pulling fs layer \n",
      "\u001b[1B258a7fcf: Pulling fs layer \n",
      "\u001b[1B6e4d269a: Pulling fs layer \n",
      "\u001b[1Be063e89c: Pulling fs layer \n",
      "\u001b[1B7417808e: Pulling fs layer \n",
      "\u001b[1B0c8f4d73: Pulling fs layer \n",
      "\u001b[9Bc00269e8: Waiting fs layer \n",
      "\u001b[11B2cc2849: Waiting fs layer \n",
      "\u001b[9Befd7921a: Waiting fs layer \n",
      "\u001b[9B258a7fcf: Waiting fs layer \n",
      "\u001b[9B6e4d269a: Waiting fs layer \n",
      "\u001b[9Be063e89c: Waiting fs layer \n",
      "\u001b[1Bb7033e58: Pulling fs layer \n",
      "\u001b[1Bf49998bc: Pulling fs layer \n",
      "\u001b[10Bc8f4d73: Waiting fs layer \n",
      "\u001b[6B48fca1de: Waiting fs layer \n",
      "\u001b[1B2019a846: Pull complete 956kB/1.956kBB\u001b[22A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[23A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[20A\u001b[2K\u001b[14A\u001b[2K\u001b[20A\u001b[2K\u001b[14A\u001b[2K\u001b[20A\u001b[2K\u001b[14A\u001b[2K\u001b[20A\u001b[2K\u001b[14A\u001b[2K\u001b[20A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[23A\u001b[2K\u001b[20A\u001b[2K\u001b[11A\u001b[2K\u001b[20A\u001b[2K\u001b[12A\u001b[2K\u001b[20A\u001b[2K\u001b[12A\u001b[2K\u001b[23A\u001b[2K\u001b[12A\u001b[2K\u001b[23A\u001b[2K\u001b[12A\u001b[2K\u001b[23A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[23A\u001b[2K\u001b[20A\u001b[2K\u001b[23A\u001b[2K\u001b[23A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[22A\u001b[2K\u001b[8A\u001b[2K\u001b[22A\u001b[2K\u001b[8A\u001b[2K\u001b[22A\u001b[2K\u001b[8A\u001b[2K\u001b[20A\u001b[2K\u001b[8A\u001b[2K\u001b[20A\u001b[2K\u001b[8A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2KDownloading  797.3MB/2.208GB\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[8A\u001b[2K\u001b[6A\u001b[2K\u001b[8A\u001b[2K\u001b[20A\u001b[2K\u001b[5A\u001b[2K\u001b[20A\u001b[2K\u001b[4A\u001b[2K\u001b[20A\u001b[2K\u001b[8A\u001b[2K\u001b[20A\u001b[2K\u001b[8A\u001b[2K\u001b[20A\u001b[2K\u001b[8A\u001b[2K\u001b[20A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[8A\u001b[2K\u001b[20A\u001b[2K\u001b[8A\u001b[2K\u001b[20A\u001b[2K\u001b[8A\u001b[2K\u001b[20A\u001b[2K\u001b[8A\u001b[2K\u001b[20A\u001b[2K\u001b[2A\u001b[2K\u001b[20A\u001b[2K\u001b[2A\u001b[2K\u001b[20A\u001b[2K\u001b[1A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2KDownloading  811.8MB/2.027GB\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[3A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2KPull complete \u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2KDigest: sha256:a85daa2907f46e70b3782818a0331df62d9b4e0b1f15f1530b2a52c8c782d46d\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "c1d599c4ae54b9773412157d64265c7b8a9a8755b76cb87da583d10294d0766d\n"
     ]
    }
   ],
   "source": [
    "TRITON_IMAGE = 'nvcr.io/nvidia/tritonserver:22.05-py3'\n",
    "!docker run --gpus all -d -p 8000:8000 -p 8001:8001 -p 8002:8002 -v {MODEL_REPO}:/models --name tritonserver {TRITON_IMAGE} tritonserver --model-repository=/models --model-control-mode=poll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccbcce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "=============================\r\n",
      "== Triton Inference Server ==\r\n",
      "=============================\r\n",
      "\r\n",
      "NVIDIA Release 22.05 (build 38317651)\r\n",
      "Triton Server Version 2.22.0\r\n",
      "\r\n",
      "Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\r\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n",
      "\r\n",
      "WARNING: CUDA Minor Version Compatibility mode ENABLED.\r\n",
      "  Using driver version 460.32.03 which has support for CUDA 11.2.  This container\r\n",
      "  was built with CUDA 11.7 and will be run in Minor Version Compatibility mode.\r\n",
      "  CUDA Forward Compatibility is preferred over Minor Version Compatibility for use\r\n",
      "  with this container but was unavailable:\r\n",
      "  [[System has unsupported display driver / cuda driver combination (CUDA_ERROR_SYSTEM_DRIVER_MISMATCH) cuInit()=803]]\r\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\r\n",
      "\r\n",
      "I0624 23:07:03.033831 1 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f4a02000000' with size 268435456\r\n",
      "I0624 23:07:03.036879 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\n",
      "I0624 23:07:03.036887 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\r\n",
      "I0624 23:07:03.036891 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 2 with size 67108864\r\n",
      "I0624 23:07:03.036894 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 3 with size 67108864\r\n",
      "I0624 23:07:03.511763 1 model_repository_manager.cc:1191] loading: example_model:1\r\n",
      "I0624 23:07:03.625258 1 initialize.hpp:43] TRITONBACKEND_Initialize: fil\r\n",
      "I0624 23:07:03.625274 1 backend.hpp:47] Triton TRITONBACKEND API version: 1.9\r\n",
      "I0624 23:07:03.625280 1 backend.hpp:52] 'fil' TRITONBACKEND API version: 1.9\r\n",
      "I0624 23:07:03.628518 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: example_model (version 1)\r\n",
      "I0624 23:07:03.631186 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: example_model_0 (GPU device 0)\r\n",
      "I0624 23:07:03.737101 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: example_model_0 (GPU device 1)\r\n",
      "I0624 23:07:03.777095 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: example_model_0 (GPU device 2)\r\n",
      "I0624 23:07:03.815714 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: example_model_0 (GPU device 3)\r\n",
      "I0624 23:07:03.855001 1 model_repository_manager.cc:1345] successfully loaded 'example_model' version 1\r\n",
      "I0624 23:07:03.855159 1 server.cc:556] \r\n",
      "+------------------+------+\r\n",
      "| Repository Agent | Path |\r\n",
      "+------------------+------+\r\n",
      "+------------------+------+\r\n",
      "\r\n",
      "I0624 23:07:03.855262 1 server.cc:583] \r\n",
      "+---------+-------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Backend | Path                                            | Config                                                                                                                                                         |\r\n",
      "+---------+-------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| fil     | /opt/tritonserver/backends/fil/libtriton_fil.so | {\"cmdline\":{\"auto-complete-config\":\"false\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\r\n",
      "+---------+-------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0624 23:07:03.855306 1 server.cc:626] \r\n",
      "+---------------+---------+--------+\r\n",
      "| Model         | Version | Status |\r\n",
      "+---------------+---------+--------+\r\n",
      "| example_model | 1       | READY  |\r\n",
      "+---------------+---------+--------+\r\n",
      "\r\n",
      "I0624 23:07:03.921230 1 metrics.cc:650] Collecting metrics for GPU 0: Tesla T4\r\n",
      "I0624 23:07:03.921261 1 metrics.cc:650] Collecting metrics for GPU 1: Tesla T4\r\n",
      "I0624 23:07:03.921285 1 metrics.cc:650] Collecting metrics for GPU 2: Tesla T4\r\n",
      "I0624 23:07:03.921291 1 metrics.cc:650] Collecting metrics for GPU 3: Tesla T4\r\n",
      "I0624 23:07:03.922222 1 tritonserver.cc:2138] \r\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Option                           | Value                                                                                                                                                                                        |\r\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| server_id                        | triton                                                                                                                                                                                       |\r\n",
      "| server_version                   | 2.22.0                                                                                                                                                                                       |\r\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\r\n",
      "| model_repository_path[0]         | /models                                                                                                                                                                                      |\r\n",
      "| model_control_mode               | MODE_POLL                                                                                                                                                                                    |\r\n",
      "| strict_model_config              | 1                                                                                                                                                                                            |\r\n",
      "| rate_limit                       | OFF                                                                                                                                                                                          |\r\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\r\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |\r\n",
      "| cuda_memory_pool_byte_size{1}    | 67108864                                                                                                                                                                                     |\r\n",
      "| cuda_memory_pool_byte_size{2}    | 67108864                                                                                                                                                                                     |\r\n",
      "| cuda_memory_pool_byte_size{3}    | 67108864                                                                                                                                                                                     |\r\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                            |\r\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                          |\r\n",
      "| strict_readiness                 | 1                                                                                                                                                                                            |\r\n",
      "| exit_timeout                     | 30                                                                                                                                                                                           |\r\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0624 23:07:03.923393 1 grpc_server.cc:4589] Started GRPCInferenceService at 0.0.0.0:8001\r\n",
      "I0624 23:07:03.923622 1 http_server.cc:3303] Started HTTPService at 0.0.0.0:8000\r\n",
      "I0624 23:07:03.965403 1 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\r\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.sleep(10)  # Wait for server to come up\n",
    "!docker logs tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543928f",
   "metadata": {},
   "source": [
    "In later sections, we'll take advantage of polling mode to make tweaks to our configuration and observe their impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f1883",
   "metadata": {},
   "source": [
    "# FAQ 4: My models are exhausting Triton's memory. What can I do?\n",
    "Tree-based models tend to have fairly modest memory needs, but when using several models together or when trying to process very large batches, you'll sometimes run into memory constraints.\n",
    "\n",
    "For models deployed on GPU, Triton allocates a device memory pool when it is launched, and the FIL backend only allocates device memory from this pool, so one option is to simply increase the size of the pool until you reach hardware limits on available memory.\n",
    "\n",
    "For models deployed on CPU or for deployments which have exceeded the hardware limits on available device memory, you may wish to instead reduce the memory consumption of models by tweaking configuration options.\n",
    "## FAQ 4.1 How can I decrease the memory consumed by a model?\n",
    "## FAQ 4.2 How do I increase Triton's memory pool?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb75955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tritonserver\r\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f tritonserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77a8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

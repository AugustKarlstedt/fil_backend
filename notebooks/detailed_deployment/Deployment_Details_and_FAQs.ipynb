{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ad97cc",
   "metadata": {},
   "source": [
    "# The FIL Backend for Triton: Deployment Details and FAQs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This example notebook focuses on the technical details of deploying tree-based models with the FIL Backend for Triton. It is organized as a series of FAQs followed by example code providing a practical illustration of the corresponding FAQ section.\n",
    "\n",
    "The goal of this notebook is to offer information that goes beyond the basics and provide answers to practical questions that may arise when attempting a real-world deployment with the FIL backend. If you are a complete newcomer to the FIL backend and are looking for a short introduction to the basics of what the FIL backend is and how to use it, you are encouraged to check out [this introductory notebook](https://github.com/triton-inference-server/fil_backend/blob/main/notebooks/categorical-fraud-detection/Fraud_Detection_Example.ipynb).\n",
    "\n",
    "While we do provide training code for example models, training models is *not* the subject of this notebook, and we will provide little detail on training. Instead, you are encouraged to use your own model(s) and data with this notebook to get a realistic picture of how your model will perform with Triton.\n",
    "\n",
    "## Hardware Pre-Requisites\n",
    "Most of this notebook is designed to run either on CPU or GPU. Sections that will only run on GPU will be marked in $\\color{#76b900}{\\text{green}}$. To guarantee that all cells will execute correctly if a GPU is not available, change `USE_GPU` in the following cell to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58491c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b761ac",
   "metadata": {},
   "source": [
    "\n",
    "## Software Pre-Requisites\n",
    "\n",
    "TODO\n",
    "\n",
    "### Bring Your Own Model\n",
    "If you are bringing your own model(s) to use with this notebook, you will need only [Docker](https://docs.docker.com/engine/install/), Numpy, and Triton's Python client package. The following command can be used to install the Python dependencies in any Python environment:\n",
    "```bash\n",
    "pip install numpy tritonclient[all]\n",
    "```\n",
    "Note that the Triton client package also requires that `libb64` be available. This can be installed on Debian-based systems via\n",
    "```bash\n",
    "sudo apt install libb64-0d\n",
    "```\n",
    "\n",
    "### Train an Example Model\n",
    "Training the example models in this notebook requires more extensive dependencies. You can install them all with the following conda environment file\n",
    "```yaml\n",
    "---\n",
    "name: triton_example\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - nvidia\n",
    "  - rapidsai\n",
    "dependencies:\n",
    "  - cudatoolkit=11.4\n",
    "  - cudf=21.12\n",
    "  - cuml=21.12\n",
    "  - cupy\n",
    "  - jupyter\n",
    "  - kaggle\n",
    "  - matplotlib\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - pip\n",
    "  - python=3.8\n",
    "  - scikit-learn\n",
    "  - pip:\n",
    "      - tritonclient[all]\n",
    "      - xgboost>=1.5,<1.6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86510373",
   "metadata": {},
   "source": [
    "# FAQ 1: What can I deploy with the FIL backend?\n",
    "The first thing you will need to begin using the FIL backend is a serialized model file. The FIL backend supports **tree-based** models serialized to formats from a variety of frameworks, including the following:\n",
    "\n",
    "## XGBoost JSON and binary models\n",
    "XGBoost uses two serialization formats, both of which are natively supported by the FIL backend. All XGBoost models except for multi-output regression models are supported.\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>VERSION NOTE:</b> Categorical variable support was added to XGBoost 1.5 as an experimental feature. The FIL backend has supported categorical variables since version 21.11.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>VERSION NOTE:</b> The XGBoost JSON format changed in XGBoost 1.6. The first version of the FIL backend to support these JSON changes will be 22.07.\n",
    "</div>\n",
    "\n",
    "## LightGBM text models\n",
    "LightGBM's text serialization format is natively supported for all LightGBM model types except for multi-output regression models.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>VERSION NOTE:</b> Models trained on categorical variables have been supported since version 21.11 of the backend\n",
    "</div>\n",
    "\n",
    "## Scikit-Learn/cuML tree models and other Treelite-supported models\n",
    "\n",
    "The FIL backend supports the following model types from Scikit-Learn/cuML:\n",
    "- GradientBoostingClassifier\n",
    "- GradientBoostingRegressor\n",
    "- IsolationForest\n",
    "- RandomForestClassifier\n",
    "- RandomForestRegressor\n",
    "- ExtraTreesClassifier\n",
    "- ExtraTreesRegressor\n",
    "\n",
    "Since Scikit-Learn and cuML do not have native serialization formats for these models (instead relying on e.g. Pickle), we use Treelite's checkpoint format to support these models. This also means that *any* framework that can export to Treelite's checkpoint format will be supported by the FIL backend. As part of this notebook, we will provide an example of how to save a Scikit-Learn or cuML model to a Treelite checkpoint.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>VERSION NOTE:</b> Treelite's checkpoint format provides no forward/backward compatibility guarantees. It is therefore <b>strongly recommended</b> that you save Scikit-Learn and cuML models to Pickle so that they can be reconverted as needed. The table below shows the version of Treelite which <b>must</b> be used with each version of the FIL backend.\n",
    "    \n",
    "<table>\n",
    "    <thead>\n",
    "        <tr><th>FIL Backend Version</th><th>Treelite</th></tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><td>21.08</td><td>1.3.0</td></tr>\n",
    "        <tr><td>21.09-21.10</td><td>2.0.0</td></tr>\n",
    "        <tr><td>21.11-22.02</td><td>2.1.0</td></tr>\n",
    "        <tr><td>22.03-22.06</td><td>2.3.0</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### FAQ 1.1 Can I deploy non-tree Scikit-Learn models like LinearRegression?\n",
    "No. The FIL backend only supports tree models and will continue to support only tree models in the future. Support for other model types may eventually be added to Triton via another backend.\n",
    "\n",
    "### FAQ 1.2 Can I deploy Scikit-Learn/cuML Pipelines with the FIL backend?\n",
    "No. If you wish to create pipelines of different models in Triton, check out Triton's [Python backend](https://github.com/triton-inference-server/python_backend#python-backend), which allows users to connect models supported by other backends with arbitrary Python logic.\n",
    "\n",
    "### FAQ 1.3 Can I deploy Scikit-Learn/cuML models serialized with Pickle?\n",
    "Pickle-serialized models can be converted to Treelite's checkpoint format using a script provided with the FIL Backend. This script is [documented here](https://github.com/triton-inference-server/fil_backend/blob/main/SKLearn_and_cuML.md#converting-to-treelite-checkpoints), and an example of its use will be included with this notebook. **Pickle models MUST be converted to Treelite checkpoints. They CANNOT be used directly by the FIL backend.**\n",
    "\n",
    "### FAQ 1.4 Can I deploy Scikit-Learn/cuML models serialized with Joblib?\n",
    "JobLib-serialized models can be loaded in Python and serialized to Treelite checkpoints. At the moment, the conversion scripts for Pickle-serialized models do **not** work with Joblib, but support for Joblib will be added with a later version. **Joblib models MUST be converted to Treelite checkpoints. They CANNOT be used directly by the FIL backend.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038cd3fa",
   "metadata": {},
   "source": [
    "# Example 1: Model Serialization\n",
    "\n",
    "In the following example code snippets, we will demonstrate how model serialization works for each of the supported model types. In the cell below, indicate the type of model you would like to use.\n",
    "\n",
    "If you are bringing your own model, please also provide the path to the serialized model. Otherwise, a model will be trained on random data in your selected format.\n",
    "\n",
    "In addition to information on where and how the model is stored, we'll use the following cell to gather a bit of metadata on the model which we'll need later on including the number of features the model expects and the number of classes it outputs. If you are using a regression model, use `1` for the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977f2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowed values for MODEL_FORMAT are xgboost_json, xgboost_bin, lightgbm, skl_pkl, cuml_pkl, skl_joblib,\n",
    "# and treelite\n",
    "MODEL_FORMAT = 'xgboost_json'\n",
    "\n",
    "# If a path is provided to a model in the specified format, that model will be used for the following examples.\n",
    "# Otherwise, if MODEL_PATH is left as None, a model will be trained and stored to a default location.\n",
    "MODEL_PATH = None\n",
    "\n",
    "# Set this value to the number of features (columns) in your dataset\n",
    "NUM_FEATURES = 32\n",
    "\n",
    "# Set this value to the number of possible output classes or 1 for regression models\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c5b43",
   "metadata": {},
   "source": [
    "## Model Training/Loading\n",
    "\n",
    "In this section, if a model path has been provided, we will load the model so that we can compare its output to what we get from Triton later in the notebook. If a model path has **not** been provided, a model of the indicated type will be trained and serialized to a default location. We will not provide detail or commentary on training, since this is not the focus of this notebook. Consult documentation or examples for your chosen framework if you would like to learn more about the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49760159",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e483d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "# Create random dataset. Even if we do not use this dataset for training, we will use it for testing later.\n",
    "# If you would like to use a real dataset, load it here into X and y Pandas dataframes\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=NUM_FEATURES,\n",
    "    n_informative=max(NUM_FEATURES // 3, 1),\n",
    "    n_classes=NUM_CLASSES,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03943d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameters for any models we need to train\n",
    "NUM_TREES = 500\n",
    "MAX_DEPTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8a2a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "# XGBoost\n",
    "def train_xgboost(X, y, n_trees=NUM_TREES, max_depth=MAX_DEPTH):\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    if USE_GPU:\n",
    "        tree_method = 'gpu_hist'\n",
    "        predictor = 'gpu_predictor'\n",
    "    else:\n",
    "        tree_method = 'hist'\n",
    "        predictor = 'cpu_predictor'\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        eval_metric='error',\n",
    "        objective='binary:logistic',\n",
    "        tree_method=tree_method,\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_trees,\n",
    "        use_label_encoder=False,\n",
    "        predictor=predictor\n",
    "    )\n",
    "    \n",
    "    return model.fit(X, y)\n",
    "\n",
    "def train_lightgbm(X, y, n_trees=NUM_TREES, max_depth=MAX_DEPTH):\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    lgb_data = lgb.Dataset(X, y)\n",
    "    \n",
    "    if classes <= 2:\n",
    "        classes = 1\n",
    "        objective = 'binary'\n",
    "        metric = 'binary_logloss'\n",
    "    else:\n",
    "        objective = 'multiclass'\n",
    "        metric = 'multi_logloss'\n",
    "    training_params = {\n",
    "        'metric': metric,\n",
    "        'objective': objective,\n",
    "        'num_class': NUM_CLASSES,\n",
    "        'max_depth': max_depth,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    return lgb.train(training_params, lgb_data, n_trees)\n",
    "\n",
    "def train_cuml(X, y, n_trees=NUM_TREES, max_depth=MAX_DEPTH):\n",
    "    from cuml.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(\n",
    "        max_depth=max_depth, n_estimators=n_trees, random_state=RANDOM_SEED\n",
    "    )\n",
    "    return model.fit(X, y)\n",
    "\n",
    "def train_skl(X, y, n_trees=NUM_TREES, max_depth=MAX_DEPTH):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(\n",
    "        max_depth=max_depth, n_estimators=n_trees, random_state=RANDOM_SEED\n",
    "    )\n",
    "    return model.fit(X, y)\n",
    "    \n",
    "    \n",
    "if MODEL_FORMAT in ('xgboost_json', 'xgboost_bin'):\n",
    "    if MODEL_PATH is not None:\n",
    "        # Load model just as a reference for later\n",
    "        import xgboost as xgb\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(MODEL_PATH)\n",
    "        print('Congratulations! Your model is already in a natively-supported format')\n",
    "    else:\n",
    "        model = train_xgboost(X, y)\n",
    "elif MODEL_FORMAT == 'lightgbm':\n",
    "    if MODEL_PATH is not None:\n",
    "        # Load model just as a reference for later\n",
    "        import lightgbm as lgb\n",
    "        model = lgb.Booster(model_file=MODEL_PATH)\n",
    "        print('Congratulations! Your model is already in a natively-supported format')\n",
    "    else:\n",
    "        model = train_lightgbm(X, y)\n",
    "elif MODEL_FORMAT in ('cuml_pkl', 'skl_pkl', 'cuml_joblib', 'skl_joblib'):\n",
    "    if MODEL_PATH is not None:\n",
    "        if MODEL_FORMAT in ('cuml_pkl', 'skl_pkl'):\n",
    "            # Load model just as a reference for later\n",
    "            import pickle\n",
    "            model = pickle.load(MODEL_PATH)\n",
    "            print(\n",
    "                \"While pickle files are not natively supported, we will use a script to\"\n",
    "                \" convert your model to a Treelite checkpoint later in this notebook.\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"Loading model from joblib file in order to convert it to Treelite checkpoint...\")\n",
    "            import joblib\n",
    "            model = joblib.load(MODEL_PATH)\n",
    "    elif MODEL_FORMAT.startswith('cuml'):\n",
    "        model = train_cuml(X, y)\n",
    "    elif MODEL_FORMAT.startswith('skl'):\n",
    "        model = train_skl(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b5b9e4",
   "metadata": {},
   "source": [
    "## The Model Repository\n",
    "Triton expects models to be stored in a specific directory structure. We will go ahead and create this directory structure now and serialize our models directly into the final directory or copy the serialized model there if the trained model was provided.\n",
    "\n",
    "Each model requires a configuration file stored in `$MODEL_REPO/$MODEL_NAME/config.pbtxt`, and a model file stored in `$MODEL_REPO/$MODEL_NAME/$MODEL_VERSION/$MODEL_FILENAME`. Note that Triton supports storing multiple versions of a model directories with different `$MODEL_VERSION` numbers starting from `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39eb4a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "MODEL_NAME = 'example_model'\n",
    "MODEL_VERSION = 1\n",
    "MODEL_REPO = os.path.abspath('data/model_repository')\n",
    "MODEL_DIR = os.path.join(MODEL_REPO, MODEL_NAME)\n",
    "VERSIONED_DIR = os.path.join(MODEL_DIR, str(MODEL_VERSION))\n",
    "\n",
    "os.makedirs(VERSIONED_DIR, exist_ok=True)\n",
    "\n",
    "# We will use the following variables to record information from the serialization\n",
    "# process that we will require later\n",
    "model_path = None\n",
    "model_format = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b396b3",
   "metadata": {},
   "source": [
    "## Example 1.1: Serializing an XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54bb67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FORMAT == 'xgboost_json':\n",
    "    # This is the default filename expected for XGBoost JSON models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_basename = 'xgboost.json'\n",
    "    model_path = os.path.join(VERSIONED_DIR, model_basename)\n",
    "    \n",
    "    model_format = 'xgboost_json'\n",
    "elif MODEL_FORMAT == 'xgboost_bin':\n",
    "    # This is the default filename expected for XGBoost binary models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_basename = 'xgboost.model'\n",
    "    model_path = os.path.join(VERSIONED_DIR, model_basename)\n",
    "    \n",
    "    # This is the format name Triton uses to indicate XGBoost binary models\n",
    "    model_format = 'xgboost'\n",
    "\n",
    "if MODEL_FORMAT.startswith('xgboost'):\n",
    "    if MODEL_PATH is not None:  # Just need to copy existing file...\n",
    "        shutil.copy(MODEL_PATH, model_path)\n",
    "    else:\n",
    "        model.save_model(model_path)  # XGB derives format from extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d87bb",
   "metadata": {},
   "source": [
    "## Example 1.2 Serializing a LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c82643",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FORMAT == 'lightgbm':\n",
    "    # This is the default filename expected for LightGBM text models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_basename = 'model.txt'\n",
    "    model_path = os.path.join(VERSIONED_DIR, model_basename)\n",
    "    \n",
    "    model_format = MODEL_FORMAT\n",
    "    \n",
    "    if MODEL_PATH is not None:  # Just need to copy existing file...\n",
    "        shutil.copy(MODEL_PATH, model_path)\n",
    "    else:\n",
    "        model.save_model(model_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7eb0d",
   "metadata": {},
   "source": [
    "## Example 1.3 Serializing an in-memory Scikit-Learn model\n",
    "<a id='example_1.3'></a>The following will show how to serialize a SKL model from Python directly to a Treelite checkpoint format. This could be a model that you have just trained or a model that you have e.g. loaded from Joblib. Again it is strongly recommended that you **save trained models in Pickle/Joblib as well as Treelite** since Treelite provides no compatibility guarantees between versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "110924ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and MODEL_FORMAT.startswith('skl'):\n",
    "    import pickle\n",
    "    archival_path = os.path.join(VERSIONED_DIR, 'model.pkl')\n",
    "    pickle.dump(model, archival_path)  # Create archival pickled version\n",
    "    \n",
    "    # This is the default filename expected for Treelite checkpoint models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_basename = 'checkpoint.tl'\n",
    "    model_path = os.path.join(VERSIONED_DIR, model_basename)\n",
    "    \n",
    "    model_format = 'treelite_checkpoint'\n",
    "    \n",
    "    import treelite\n",
    "    tl_model = treelite.sklearn.import_model(model)\n",
    "    tl_model.serialize(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70cc57",
   "metadata": {},
   "source": [
    "## Example 1.4 Serializing an in-memory cuML model\n",
    "<a id='example_1.4'></a>The following will show how to serialize a cuML model from Python directly to a Treelite checkpoint format. This could be a model that you have just trained or a model that you have e.g. loaded from Joblib. Again it is strongly recommended that you **save trained models in Pickle/Joblib as well as Treelite** since Treelite provides no compatibility guarantees between versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16c0fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and MODEL_FORMAT.startswith('cuml'):\n",
    "    import pickle\n",
    "    archival_path = os.path.join(VERSIONED_DIR, 'model.pkl')\n",
    "    pickle.dump(model, archival_path)  # Create archival pickled version\n",
    "    \n",
    "    # This is the default filename expected for Treelite checkpoint models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_basename = 'checkpoint.tl'\n",
    "    model_path = os.path.join(VERSIONED_DIR, model_basename)\n",
    "    \n",
    "    model_format = 'treelite_checkpoint'\n",
    "    \n",
    "    model.convert_to_treelite_model().to_treelite_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a73ee",
   "metadata": {},
   "source": [
    "## Example 1.5 Converting a pickled Scikit-Learn model\n",
    "For convenience, the FIL backend provides a script which can be used to convert a pickle file containing a Scikit-Learn model directly to a Treelite checkpoint file. If you do not have access to that script or prefer to work directly from Python, you can always load the pickled model into memory and then serialize it as in [Example 1.3](#example_1.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bbf2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_PATH is not None and MODEL_FORMAT == 'skl_pkl':\n",
    "    archival_path = os.path.join(VERSIONED_DIR, 'model.pkl')\n",
    "    shutil.copy(MODEL_PATH, archival_path)\n",
    "    \n",
    "    !../../scripts/convert_sklearn {archival_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adadf2ce",
   "metadata": {},
   "source": [
    "## Example 1.6 Converting a pickled cuML model\n",
    "For convenience, the FIL backend provides a script which can be used to convert a pickle file containing a cuML model directly to a Treelite checkpoint file. If you do not have access to that script or prefer to work directly from Python, you can always load the pickled model into memory and then serialize it as in [Example 1.4](#example_1.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b22bfb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_PATH is not None and MODEL_FORMAT == 'cuml_pkl':\n",
    "    archival_path = os.path.join(VERSIONED_DIR, 'model.pkl')\n",
    "    shutil.copy(MODEL_PATH, archival_path)\n",
    "    \n",
    "    !python ../../scripts/convert_cuml.py {archival_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f5015",
   "metadata": {},
   "source": [
    "# FAQ 2: How do I execute models on CPU only? On GPU?\n",
    "\n",
    "In addition to a serialized model file, you must provide a `config.pbtxt` configuration file for each model you wish to serve with the FIL backend for Triton. Within that file, it is possible to specify whether a model will run on CPU or GPU and how many instances of the model you wish to serve. For example, adding the following entry to the configuration file will create one instance of the model for each available GPU and run those instances each on their own dedicated GPU:\n",
    "\n",
    "```pbtxt\n",
    "  instance_group [{ kind: KIND_GPU }]\n",
    "```\n",
    "\n",
    "If you wish to instead run exactly three instances on CPU, the following entry can be used:\n",
    "```pbtxt\n",
    "  instance_group [\n",
    "    {\n",
    "      count: 3\n",
    "      kind: KIND_CPU\n",
    "    }\n",
    "  ]\n",
    "```\n",
    "\n",
    "In the following example, we will create a configuration file that can be used to serve your model on either CPU or GPU depending on the value of the `USE_GPU` flag set earlier in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c067fd",
   "metadata": {},
   "source": [
    "# Example 2: Generating a configuration file\n",
    "\n",
    "Based on the information provided about your model in previous cells, we can now construct a `config.pbtxt` that can be used to run that model on Triton. We will generate the configuration text and save it to the appropriate location.\n",
    "\n",
    "For full information on configuration options, check out the FIL backend [documentation](https://github.com/triton-inference-server/fil_backend#configuration). For a detailed example of configuration file construction, you can also check out the [introductory notebook](https://nbviewer.org/github/triton-inference-server/fil_backend/blob/main/notebooks/categorical-fraud-detection/Fraud_Detection_Example.ipynb#The-Configuration-File)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c1be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum size in bytes for input and output arrays\n",
    "MAX_MEMORY_BYTES = 60_000_000\n",
    "bytes_per_sample = (NUM_FEATURES + NUM_CLASSES) * 4\n",
    "max_batch_size = MAX_MEMORY_BYTES // bytes_per_sample\n",
    "\n",
    "# Select deployment hardware (GPU or CPU)\n",
    "if USE_GPU:\n",
    "    instance_kind = 'KIND_GPU'\n",
    "else:\n",
    "    instance_kind = 'KIND_CPU'\n",
    "\n",
    "config_text = f\"\"\"backend: \"fil\",\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {NUM_FEATURES} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {NUM_CLASSES} ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"xgboost_json\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"AUTO\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{}}\"\"\"\n",
    "\n",
    "config_path = os.path.join(MODEL_DIR, 'config.pbtxt')\n",
    "with open(config_path, 'w') as file_:\n",
    "    file_.write(config_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590f316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca1856e",
   "metadata": {},
   "source": [
    "## A Note on Categorical Variables\n",
    "Categorical variable support was added to the Triton FIL backend in release 21.12 and to XGBoost in release 1.5. If you would like to use an earlier version of either of these or if you simply wish to see how the same workflow would go without explicit categorical variable support, you may set the `USE_CATEGORICAL` variable in the following cell to `false`. Otherwise, by leaving it as `True`, you can take advantage of categorical variable support.\n",
    "\n",
    "Please note that categorical variable support is still considered experimental in XGBoost 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3205ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CATEGORICAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRITON_IMAGE = 'nvcr.io/nvidia/tritonserver:21.12-py3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190453b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull {TRITON_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ae4a2",
   "metadata": {},
   "source": [
    "## Fetching Training Data\n",
    "For this example, we will make use of data from the [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection/overview) Kaggle competition. You may fetch the data from this competition using the Kaggle command line client using the following commands.\n",
    "\n",
    "\n",
    "**NOTE**: You will need to make sure that your Kaggle credentials are [available](https://github.com/Kaggle/kaggle-api#api-credentials) either through a kaggle.json file or via environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd826c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c ieee-fraud-detection\n",
    "!unzip -u ieee-fraud-detection.zip\n",
    "train_csv = 'train_transaction.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0fb66a",
   "metadata": {},
   "source": [
    "## Training Example Models\n",
    "While the IEEE-CIS Kaggle competition focused on a more sophisticated problem involving analysis of both fraudulent transactions and the users linked to those transactions, we will use a simpler version of that problem (identifying fraudulent transactions only) to build our example model. In the following steps, we make use of cuML's preprocessing tools to clean the data and then train two example models using XGBoost. Note that we will be making use of the new categorical feature support in XGBoost 1.5. If you wish to use an earlier version of XGBoost, you will need to perform a [label encoding](https://docs.rapids.ai/api/cuml/stable/api.html?highlight=labelencoder#cuml.preprocessing.LabelEncoder.LabelEncoder) on the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.preprocessing import SimpleImputer\n",
    "if not USE_CATEGORICAL:\n",
    "    from cuml.preprocessing import LabelEncoder\n",
    "# Due to an upstream bug, cuML's train_test_split function is\n",
    "# currently non-deterministic. We will therefore use sklearn's\n",
    "# train_test_split in this example to obtain more consistent\n",
    "# results.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b440e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV files into cuDF DataFrames\n",
    "data = cudf.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de552313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs in data\n",
    "nan_columns = data.columns[data.isna().any().to_pandas()]\n",
    "float_nan_subset = data[nan_columns].select_dtypes(include='float64')\n",
    "\n",
    "imputer = SimpleImputer(missing_values=cp.nan, strategy='median')\n",
    "data[float_nan_subset.columns] = imputer.fit_transform(float_nan_subset)\n",
    "\n",
    "obj_nan_subset = data[nan_columns].select_dtypes(include='object')\n",
    "data[obj_nan_subset.columns] = obj_nan_subset.fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string columns to categorical or perform label encoding\n",
    "cat_columns = data.select_dtypes(include='object')\n",
    "if USE_CATEGORICAL:\n",
    "    data[cat_columns.columns] = cat_columns.astype('category')\n",
    "else:\n",
    "    for col in cat_columns.columns:\n",
    "        data[col] = LabelEncoder().fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = data.drop('isFraud', axis=1)\n",
    "y = data.isFraud.astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.to_pandas(), y.to_pandas(), test_size=0.3, stratify=y.to_pandas(), random_state=SEED\n",
    ")\n",
    "# Copy data to avoid slowdowns due to fragmentation\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model training function\n",
    "def train_model(num_trees, max_depth):\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        enable_categorical=USE_CATEGORICAL,\n",
    "        use_label_encoder=False,\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        objective='binary:logistic',\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=num_trees\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_test, y_test)]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc239af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a small model with just 500 trees and a maximum depth of 3\n",
    "small_model = train_model(500, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a large model with 5000 trees and a maximum depth of 12\n",
    "large_model = train_model(5000, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some room on the GPU by explicitly deleting dataframes\n",
    "import gc\n",
    "del data\n",
    "del nan_columns\n",
    "del float_nan_subset\n",
    "del imputer\n",
    "del obj_nan_subset\n",
    "del cat_columns\n",
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3008720",
   "metadata": {},
   "source": [
    "## Deploying Models in Triton\n",
    "Now that we have two example models to work with, let's actually deploy them for real-time serving using Triton. In order to do so, we will need to first serialize the models in the directory structure that Triton expects and then add configuration files to tell Triton exactly how we wish to use these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e0a5c",
   "metadata": {},
   "source": [
    "### Model Serialization\n",
    "Triton models can be stored locally on disk or in S3, Google Cloud Storage, or Azure Storage. For this example, we will stick to local storage, but information about using cloud storage solutions can be found [here](https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md). Each model has a dedicated directory within a main model repository directory. Multiple versions of a model can also be served by Triton, as indicated by numbered directories (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a005e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7126325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model repository directory. The name of this directory is arbitrary.\n",
    "REPO_PATH = os.path.abspath('model_repository')\n",
    "os.makedirs(REPO_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db311a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_model(model, model_name):\n",
    "    # The name of the model directory determines the name of the model as reported\n",
    "    # by Triton\n",
    "    model_dir = os.path.join(REPO_PATH, model_name)\n",
    "    # We can store multiple versions of the model in the same directory. In our\n",
    "    # case, we have just one version, so we will add a single directory, named '1'.\n",
    "    version_dir = os.path.join(model_dir, '1')\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    # The default filename for XGBoost models saved in json format is 'xgboost.json'.\n",
    "    # It is recommended that you use this filename to avoid having to specify a\n",
    "    # name in the configuration file.\n",
    "    model_file = os.path.join(version_dir, 'xgboost.json')\n",
    "    model.save_model(model_file)\n",
    "    \n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d42c83",
   "metadata": {},
   "source": [
    "We will be deploying two copies of each of our example models: one on CPU and one on GPU. We will use these separate instances to demonstrate the performance differences between GPU and CPU execution later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_dir = serialize_model(small_model, 'small_model')\n",
    "small_model_cpu_dir = serialize_model(small_model, 'small_model-cpu')\n",
    "large_model_dir = serialize_model(large_model, 'large_model')\n",
    "large_model_cpu_dir = serialize_model(large_model, 'large_model-cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550c7a9",
   "metadata": {},
   "source": [
    "### The Configuration File\n",
    "The configuration file associated with a model tells Triton a little bit about the model itself and how you would like to use it. You can read about all generic Triton configuration options [here](https://github.com/triton-inference-server/server/blob/master/docs/model_configuration.md) and about configuration options specific to the FIL backend [here](https://github.com/triton-inference-server/fil_backend#configuration), but we will focus on just a few of the most common and relevant options in this example. Below are general descriptions of these options:\n",
    "- **max_batch_size**: The maximum batch size that can be passed to this model. In general, the only limit on the size of batches passed to a FIL backend is the memory available with which to process them. For GPU execution, the available memory is determined by the size of Triton's CUDA memory pool, which can be set via a command line argument when starting the server.\n",
    "- **input**: Options in this section tell Triton the number of features to expect for each input sample.\n",
    "- **output**: Options in this section tell Triton how many output values there will be for each sample. If the \"predict_proba\" option (described further on) is set to true, then a probability value will be returned for each class. Otherwise, a single value will be returned indicating the class predicted for the given sample.\n",
    "- **instance_group**: This determines how many instances of this model will be created and whether they will use the GPU or CPU.\n",
    "- **model_type**: A string indicating what format the model is in (\"xgboost_json\" in this example, but \"xgboost\", \"lightgbm\", and \"tl_checkpoint\" are valid formats as well).\n",
    "- **predict_proba**: If set to true, probability values will be returned for each class rather than just a class prediction.\n",
    "- **output_class**: True for classification models, false for regression models.\n",
    "- **threshold**: A score threshold for determining classification. When output_class is set to true, this must be provided, although it will not be used if predict_proba is also set to true.\n",
    "- **storage_type**: In general, using \"AUTO\" for this setting should meet most usecases. If \"AUTO\" storage is selected, FIL will load the model using either a sparse or dense representation based on the approximate size of the model. In some cases, you may want to explicitly set this to \"SPARSE\" in order to reduce the memory footprint of large models.\n",
    "\n",
    "Based on this information, let's set up configuration files for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fd518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum size in bytes for input and output arrays. If you are\n",
    "# using Triton 21.11 or higher, all memory allocations will make\n",
    "# use of Triton's memory pool, which has a default size of\n",
    "# 67_108_864 bytes. This can be increased using the\n",
    "# `--cuda-memory-pool-byte-size` option when the server is\n",
    "# started, but this notebook should work fine with default\n",
    "# settings.\n",
    "MAX_MEMORY_BYTES = 60_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ab7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_test.shape[1]\n",
    "num_classes = cp.unique(y_test).size\n",
    "bytes_per_sample = (NUM_FEATURES * NUM_CLASSES) * 4\n",
    "max_batch_size = MAX_MEMORY_BYTES // bytes_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e794668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_config(model_dir, deployment_type='gpu', storage_type='AUTO'):\n",
    "    if deployment_type.lower() == 'cpu':\n",
    "        instance_kind = 'KIND_CPU'\n",
    "    else:\n",
    "        instance_kind = 'KIND_GPU'\n",
    "\n",
    "    config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {features} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {num_classes} ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"xgboost_json\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"{storage_type}\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\"\"\"\n",
    "    config_path = os.path.join(model_dir, 'config.pbtxt')\n",
    "    with open(config_path, 'w') as file_:\n",
    "        file_.write(config_text)\n",
    "\n",
    "    return config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43016177",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config(small_model_dir, deployment_type='gpu')\n",
    "generate_config(small_model_cpu_dir, deployment_type='cpu')\n",
    "generate_config(large_model_dir, deployment_type='gpu')\n",
    "generate_config(large_model_cpu_dir, deployment_type='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016e3d5",
   "metadata": {},
   "source": [
    "### Starting the server\n",
    "With valid models and configuration files in place, we can now start the server. Below, we do so, use the Python client to wait for it to come fully online, and then check the logs to make sure we didn't get any unexpected warnings or errors while loading the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus all -d -p 8000:8000 -p 8001:8001 -p 8002:8002 -v {REPO_PATH}:/models --name tritonserver {TRITON_IMAGE} tritonserver --model-repository=/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tritonclient.grpc as triton_grpc\n",
    "from tritonclient import utils as triton_utils\n",
    "HOST = 'localhost'\n",
    "PORT = 8001\n",
    "TIMEOUT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c7972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = triton_grpc.InferenceServerClient(url=f'{HOST}:{PORT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66742762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for server to come online\n",
    "server_start = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        if client.is_server_ready() or time.time() - server_start > TIMEOUT:\n",
    "            break\n",
    "    except triton_utils.InferenceServerException:\n",
    "        pass\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker logs tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ae4ef",
   "metadata": {},
   "source": [
    "## Submitting inference requests\n",
    "With our models now deployed on a running Triton server, let's confirm that we get the same results from the deployed model as we get locally. Note that we will occasionally see slight divergences due to floating point errors during parallel execution, but otherwise, results should match.\n",
    "\n",
    "### Categorical variables\n",
    "If you are using a model with categorical features, a certain amount of care must be taken with categorical features, just as if you were executing a model locally. Both XGBoost and LightGBM depend on the input data frames to convert categories into numeric variables. If data is later submitted from a data frame which contains a different subset of categories, this numeric conversion will not be handled properly. In this example, we will use the same dataframe we used during testing, so we need not consider this, but otherwise we would need to note the mapping used for the `.codes` attribute for each categorical feature in the training dataframe and make sure the same codes were used when submitting inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64db65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def convert_to_numpy(df):\n",
    "    df = df.copy()\n",
    "    cat_cols = df.select_dtypes('category').columns\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].cat.codes\n",
    "    for col in df.columns:\n",
    "        df[col] =  pd.to_numeric(df[col], downcast='float')\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60633965",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = convert_to_numpy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ceb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_predict(model_name, arr):\n",
    "    triton_input = triton_grpc.InferInput('input__0', arr.shape, 'FP32')\n",
    "    triton_input.set_data_from_numpy(arr)\n",
    "    triton_output = triton_grpc.InferRequestedOutput('output__0')\n",
    "    response = client.infer(model_name, model_version='1', inputs=[triton_input], outputs=[triton_output])\n",
    "    return response.as_numpy('output__0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_result = triton_predict('small_model', np_data[0:5])\n",
    "local_result = small_model.predict_proba(X_test[0:5])\n",
    "print(\"Result computed on Triton: \")\n",
    "print(triton_result)\n",
    "print(\"\\nResult computed locally: \")\n",
    "print(local_result)\n",
    "cp.testing.assert_allclose(triton_result, local_result, rtol=1e-6, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8147db66",
   "metadata": {},
   "source": [
    "## Optimizing Performance\n",
    "Triton offers several tools to help tune your model deployment parameters and optimize your target metrics, whether that be throughput, latency, device utilization, or some other measure of performance. Some of these optimizations depend on expected server load and whether inference requests will be submitted in batches or one at a time from clients. As we shall see, Triton's performance analysis tools allow you to test performance based on a wide range of anticipated scenarios and modify deployment parameters accordingly.\n",
    "\n",
    "For this example, we will make use of Triton's `perf_analyzer` [tool](https://github.com/triton-inference-server/server/blob/main/docs/perf_analyzer.md#performance-analyzer), which allows us to quickly measure throughput and latency based on different batch sizes and request concurrency. We'll start with a basic comparison of the performance of our large model deployed on CPU vs GPU with batch size 1 and no concurrency.\n",
    "\n",
    "All of the specific performance numbers here were obtained on a DGX-1 with 8 V100s and Triton 21.11, but your numbers may vary depending on available hardware and whether or not you chose to enable categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f6dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance of our large model on CPU.\n",
    "# By default, perf_analyzer uses batch size 1 and concurrency 1.\n",
    "!perf_analyzer -m large_model-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now get the same performance numbers for GPU execution\n",
    "!perf_analyzer -m large_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b953ec1",
   "metadata": {},
   "source": [
    "Already, we can see that GPU execution offers substantially improved throughput at lower latency for this complex model, but let's see what happens when we look at higher batch sizes or request load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance with batch size 6 and a concurrrency of 6 for\n",
    "# request submissions\n",
    "!perf_analyzer -m large_model-cpu -b 6 --concurrency-range 6:6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf_analyzer -m large_model -b 6 --concurrency-range 6:6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6705f",
   "metadata": {},
   "source": [
    "As we can see, deployed on CPU, the model was able to offer a somewhat increased throughput at higher load, but latency increased dramatically. Meanwhile, the same model deployed on the GPU significantly increased its throughput with only a slight increase in latency.\n",
    "\n",
    "In order to maintain a tight latency budget on a CPU-only server under high request load, we would have to turn to a significantly less sophisticated model. Let's imagine that we were trying to keep our p99 latency under 2 ms on the DGX machine referred to above. On CPU, we can just barely stay under that budget with a batch size of 6 and concurrency of 6 on CPU. Deploying the same model on GPU with the same parameters, we can keep our p99 latency under 0.7 ms and offer 3.5X the throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba9337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!perf_analyzer -m small_model-cpu -b 6 --concurrency-range 6:6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a5592",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf_analyzer -m small_model -b 6 --concurrency-range 6:6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9beba",
   "metadata": {},
   "source": [
    "Let's see how far we can push our large model on GPU while staying within our 2 ms latency budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ab7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf_analyzer -m large_model -b 80 --concurrency-range 8:8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956b745",
   "metadata": {},
   "source": [
    "On the GPU, this larger model can achieve 20X the throughput of the smaller model on CPU, allowing us to handle a substantially higher load. But of course throughput performance is only part of the picture. If our latency budget forces us to use a smaller model on CPU, how much worse will we do at actually detecting fraud? Let's compute results for the entire test dataset using the large and small models and then compare their precision-recall curves to see how much we may be losing by resorting to the smaller model for CPU deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f026d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424da1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_COUNT = 8\n",
    "\n",
    "def create_batches(arr):\n",
    "    # Determine how many chunks are needed to keep size <= max_batch_size\n",
    "    chunks = (\n",
    "        arr.shape[0] // max_batch_size +\n",
    "        int(bool(arr.shape[0] % max_batch_size) or arr.shape[0] < max_batch_size)\n",
    "    )\n",
    "    return np.array_split(arr, max(GPU_COUNT, chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431cdd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time large_model_results = np.concatenate([triton_predict('large_model', chunk) for chunk in create_batches(np_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time small_model_results = np.concatenate([triton_predict('small_model-cpu', chunk) for chunk in create_batches(np_data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3862ec0",
   "metadata": {},
   "source": [
    "Note that we can more quickly process the full dataset on GPU even with a significantly more sophisticated model  than we are using for our CPU deployment. As an interesting point of comparison, due to the optimized inference performance of the RAPIDS Forest Inference Library (FIL) used by the Triton backend and Triton's inherent ability to parallelize over available GPUs, it is even faster to submit these samples for processing to Triton than it is to process them locally using XGBoost for the larger model, despite the overhead of data transfer. For information about invoking FIL directly in Python without Triton, see the [FIL documentation](https://github.com/rapidsai/cuml/tree/branch-21.12/python/cuml/fil#fil---rapids-forest-inference-library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0094559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time large_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a8bb1",
   "metadata": {},
   "source": [
    "We now return to evaluating the benefit of the larger model for accurately detecting fraud by computing precision-recall curves for both the small and large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_precision, large_recall, _ = cuml.metrics.precision_recall_curve(y_test, large_model_results[:, 1])\n",
    "small_precision, small_recall, _ = cuml.metrics.precision_recall_curve(y_test, small_model_results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a855acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fe250",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(small_precision, small_recall, color='#0071c5')\n",
    "plt.plot(large_precision, large_recall, color='#76b900')\n",
    "plt.title('Precision vs Recall for Small and Large Models')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36712d5b",
   "metadata": {},
   "source": [
    "As we can see, the larger, more sophisticated model dominates the smaller model all along this curve. By deploying our model on GPU, we can identify a far greater proportion of actual fraud incidents with fewer false positives, all without going over our latency budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the server\n",
    "!docker rm -f tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747b1fb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this example notebook, we showed how to deploy an XGBoost model in Triton using the new FIL backend. While it is possible to deploy these models on both CPU and GPU in Triton, GPU-deployed models offer far higher throughput at lower latency. As a result, we can deploy more sophisticated models on the GPU for any given latency budget and thereby obtain far more accurate results.\n",
    "\n",
    "While we have focused on XGBoost in this example, FIL also natively supports LightGBM's text serialization format as well as Treelite's checkpoint format. Thus, the same general steps can be used to serve LightGBM models and any Treelite-convertible model (including Scikit-Learn and cuML forest models). With the new FIL backend, Triton is now ready to serve forest models of all kinds in production, whether on their own or in concert with any of the deep-learning models supported by Triton."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
